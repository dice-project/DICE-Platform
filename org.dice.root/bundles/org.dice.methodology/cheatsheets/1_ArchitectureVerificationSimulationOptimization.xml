<?xml version="1.0" encoding="UTF-8"?>
<cheatsheet
      title="Architecture Verification, Simulation and Optimization">
   <intro>
      <description>
      	Find a service-oriented architecture that meets the requirements.
      </description>
   </intro>
   <item
         title="Architecture Design">
      <description>In the architecture simulation and verification scenario, Developers get from System Orchestrators a list of requirements about the system to construct, and want to study models of possible production environments in order to predict behaviors and performances for different implementation strategies. This work is similar to the one done by engineering or consultancy offices for civil or aeronautical projects, such as the construction of a building, or the manufacturing of an aircraft. However, in these disciplines, the characteristics of the environment are imposed by natural physical conditions; whereas in software development, the characteristics of the production environment are negotiated with Operators. This use case is enabled thanks to the DICE/UML profile, the simulation tool, and the verification tool. Here are the stages to follow.follow.
      </description>
      <subitem
            label="The DICE Methodology promotes the service-oriented architectural style. In service orientation, an application is designed as an aggregate of software components using and/or delivering services to other software components, typically over a network, via carefully-defined application programming interfaces (API), implemented on top of some communication protocols (e.g, HTTP). These APIs enable a client component to remotely activate a precise service’s behavior.">
      </subitem>
      <subitem
            label="Following service-orientation principles, the first step of The DICE Methodology for the architecture verification and simulation scenario—the service-oriented architecture design —consists in modeling a solution in terms of software components (or software nodes) serving and/or being ²served by other software components. For this activity, the modeling language endorsed by DICE is the UML Object diagram. The design can be accomplished both in a platform-independent and technology-specific manner by using respectively the DPIM and DTSM profiles. At the DPIM level, components are stereotyped as source nodes, storage nodes, computation nodes, or channel nodes; underlying technologies and communication protocols are not yet determined. It is only at the DTSM level that Developers can state, for instance, that a given storage node is actually a Cassandra cluster.">
      </subitem>
      <subitem
            label="Once an architecture is envisioned, it is indispensable to classify the services thereof as internal or external. An external service must be provisioned in the production environment by Operators, while an internal service must be programmed by Developers, and deployed into it afterwards. An external service can be, for example, a software open-sourced on the Internet, or subcontracted to a company. This partitioning of services takes place during exhaustive deliberations between Developers and Operators. In other words, the DICE methodology innovates by integrating Operators into the development lifecycle from the beginning to the end of the project.">
      </subitem>
   </item>
   <item
         title="Verification">
      <description>
         Verify invariant properties.
	</description>
      <subitem
            label="Developers can invoke the verification tool to check that the services satisfy some invariant properties (Figure 4, verification). For example, the developer can verify that the buffer queue of a Storm deployment becomes limited within a computatin. More details are available at https://github.com/dice-project/DICE-Knowledge-Repository/wiki/DICE-Knowledge-Repository#verification">
         <command
               required="false"
               serialization="org.eclipse.ui.browser.openBrowser(url=https://github.com/dice-project/DICE-Knowledge-Repository/wiki/DICE-Knowledge-Repository#verification)"/>
      </subitem>
   </item>
   <item
         title="Design of the DIA Behavior">
      <description>
         Developers and Operators respectively design the behavior of internal and external services.
      </description>
      <subitem
            label="In order to proceed with further evaluations of the quality of the DIA, in particular the evaluation of the performance and the reliability of the DIA, it is necessary that the user provides a design of the system behavior. In fact, the scenario-based performance evaluation is the way to traditionally carry the computation of performance metrics.">
      </subitem>
      <subitem
            label="The behavioral design of the DIA is represented through UML behavioral and structural diagrams. In particular, Activity or Sequence Diagrams for the former and UML Deployment diagrams for the latter. The behavioural design is enhanced with stereotypes from DICE DPIM or DTSM profiles.">
      </subitem>
      <subitem
            label="In the Activity and Sequence diagrams, the user specifies the operations performed by the DIA, their relative ordering, and concepts that are typical in the description of workflows, such as the execution in parallel of a set of operations, conditional execution of operations, iterations over a set of operations, etc. The utilization of DICE profiles allows to include information that is relevant for the performance and reliability with which the DIA will execute. Some examples of pieces of relevant information are: probabilities of execution failures, execution times of single operations, configuration characteristics of the Big Data technologies used, quantity of jobs submitted to these technologies, etc.">
      </subitem>
      <subitem
            label="In turn, the structural part of the DIA design is also stereotyped using DICE DPIM or DTSM profiles. These diagrams model the assignment of the behavioural software artifacts previously defined into a simulated environment. Here, the DICE profiles allow the user to define properties of the resources used by the DIA, such as their quantity or the typical time to failure of the machines where the DIA executes.">
      </subitem>
      <subitem
            label="With this information, the simulation tool can proceed with its execution (see Figure 4, service behavior design).">
      </subitem>
   </item>
   <item
         title="Simulation">
      <description>
         Simulate the behavior of a service in a simulation environment.
      </description>
      <subitem
            label="The simulation tool is able to produce performance and reliability results for a given DIA, in fact starting from the behavioural design of such DIA (Figure 4, simulation). In particular, the simulation tool can compute specific metrics for performance and reliability, both at the DPIM and DTSM levels. When the tool is launched, the user graphically interacts to specify the kind of analysis to perform, i.e., performance or reliability.">
      </subitem>
      <subitem
            label="For performance analysis, the metrics provided refer to the service time, throughput and utilization. Then, the user can define using the DICE profile annotations that s/he is interested in assessing the expected response time of executions of the DIA, its throughput or the utilization of some resources exposed by the DIA. With this information, the user decides whether the current design satisfies the DIA performance requirements. The performance result regarding the utilization of resources provides to user with information to infer the location of the bottleneck of the DIA executions, or whether there is an under-utilization of resources and therefore DIA may waste them. ">
      </subitem>
      <subitem
            label="In case that the user had left some information in the profiled UML models as variables, this is the moment to specify their actual values. Variables in the UML models are useful for what-if performance analysis of the design, i.e., the user wants to know the system performance under different configurations. Some examples of these variables are: the number of computational resources that the DIA can use -ranging for instance from 2 to 5-, the quantity jobs submitted to the DIA -ranging for instance from 5 to 20-.">
      </subitem>
      <subitem
            label="After these phases of specifying the kind of analysis and configuring the actual values for the simulation, the user can press the Run button in the graphical interface and execute the Petri net based DIA simulation. When the execution finishes, the user receives the performance results of the DIA simulation.  The result produced by the evaluation is processed to generate a tool-independent report with the assessment of performance metrics. He can then proceed to the next step being informed of the expected performance, or modify the DIA design if the results do not satisfy the DIA performance requirements and/or expectations. ">
      </subitem>
      <subitem
            label="For reliability analysis, the process is analogous: first the user specifies the kind of reliability metrics -such as probability of failure of a concrete execution or the mean time to failure of the DIA- and configures the actual values of the information that was possibly left as variable in the design. Then press Run, the execution starts, and the reliability results of the simulation are processed provided as a tool-independent report. Then, the user decides whether to proceed with the next step or to modify the design in case that results do not satisfy the DIA reliability expectations.">
      </subitem>
      <subitem
            label="More details are available at https://github.com/dice-project/DICE-Knowledge-Repository/wiki/DICE-Knowledge-Repository#simulation-tool">
         <command
               required="false"
               serialization="org.eclipse.ui.browser.openBrowser(url=https://github.com/dice-project/DICE-Knowledge-Repository/wiki/DICE-Knowledge-Repository#simulation-tool)"/>
      </subitem>
   </item>
   <item
         title="Optimization">
      <description>
         Find the less expensive cluster configuration able to guarantee the application jobs to be executed before a user defined deadline.<br/>
         <br/>
         The step—architecture optimization (Figure 5)—consists of letting the optimization tool find an optimal deployment model. The optimization tool takes the DDSM and reorganizes the infrastructure and the technologies therein to find an optimal architecture. This tool is invoked as follows:
      </description>
      <subitem
            label="Open an already existing DICE project containing some DICE Deployment Diagram">
      </subitem>
      <subitem
            label="Select a DICE Deployment Diagram (specifying QoS constraints)">
      </subitem>
      <subitem
            label="Run Model-to-Model transformation tool (part of the simulation tool) to get the performance model">
      </subitem>
      <subitem
            label="Explore design space and run QN/PN simulation to evaluate performance metrics of the candidate solution">
      </subitem>
      <subitem
            label="Output the final DICE Deployment Diagram specifying type and number of VMs for target deployment">
      </subitem>
   </item>
</cheatsheet>
